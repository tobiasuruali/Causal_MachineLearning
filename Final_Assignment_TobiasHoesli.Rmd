---
title: "Assignment"
subtitle: "Causal ML Luzern"
author: "Tobias Hösli"
date: "`r format(Sys.time(), '%m/%y')`"
output: 
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
---

<br>

# Theory recap: Double ML (20 points)
*Briefly describe in your own words (and potentially formulas) the general idea of Double Machine Learning. What is the crucial property that makes it work? What should the nuisance parameter predictions fulfill for Double ML to work?*

<br>

Double Machine Learning (DML) is a method for estimating treatment effects in the presence of many observed covariates. Let $Y$ denote the outcome variable, $W|D$ denote the treatment variable and $X$ denote the vector of covariates. The goal of DML is to estimate the average treatment effect $\theta_0 = E[Y(1) - Y(0)]$, where $Y(d)$ denotes the potential outcome under treatment level $d \in {0, 1}$.

Classical statistical approaches may not be applicable when there are many covariates or when their effect on the treatment and outcome cannot be satisfactorily modeled by parametric functions. DML addresses this issue by using machine learning methods to estimate nuisance functions that allow us to isolate the effect of the treatment from other factors.

The first step in DML is to estimate two nuisance functions: (1) The conditional expectation function of the outcome given covariates and treatment: $\hat{\mu}(X,D) = E[Y|X,D]$; and (2) The propensity score function: $\hat{e}(X) = E[D|X]$. To account for confounding variables and lessen bias in treatment effect estimations, these nuisance parameters are used and can be estimated using flexible machine learning methods.

Next, we compute an orthogonal score function based on these estimates. For example, in a  Interactive regression model (IRM) with a binary treatment variable, we can use Neyman’s orthogonal score function: *($W$ is the data, $Y$ the outcome variable, $D$ the treatment variable and $X$ covariates)*

$$
\psi(W;\theta,\eta) := (g(1,X) - g(0,X)) + D(Y-g(1,X)) m(X) - (1-D)(Y-g(0,X)) 1-m(X)-\theta
$$
The average treatment effect (ATE) can then be estimated as (IRM):

$$
\begin{aligned}
\hat{\tau}_{ATE} &= \frac{1}{n}\sum_{i=1}^n \psi(W_i;\theta,\eta) \\
\psi(W ; \theta, \eta) & :=g(1, X)-g(0, X)+\frac{D(Y-g(1, X))}{m(X)}-\frac{(1-D)(Y-g(0, X))}{1-m(x)}-\theta \\
& =\psi_a(W ; \eta) \theta+\psi_b(W ; \eta)
\end{aligned}
$$

One advantage of DML over other methods for estimating treatment effects is its ability to handle high-dimensional data and non-parametric relationships between covariates and outcomes. This makes it well-suited for applications where there are many potential confounders or where it is difficult to specify a parametric model for their relationship with outcomes.

Double Machine Learning (DML) provides a powerful tool for estimating treatment effects in situations where traditional statistical approaches may not be applicable due to high dimensionality or complex relationships between covariates and outcomes.But for DML to work,predictions of nuisance parameters must be low-biased and meet regularity conditions. 


<br>


# Practice task: Average effects with and without instrument (60 points)

## 401(k) dataset 

We again use the data of the `hdm` package. The data was used in [Chernozhukov and Hansen (2004)](https://direct.mit.edu/rest/article/86/3/735/57586/The-Effects-of-401-K-Participation-on-the-Wealth). Their paper investigates the effect of participation in the employer-sponsored 401(k) retirement savings plan (*p401*) on net assets (*net_tfa*). Since then, the data was used to showcase many new methods. It is not the most comprehensive dataset with basically ten covariates/regressors/predictors:

- *age*: age

- *db*: defined benefit pension

- *educ*: education (in years)

- *fsize*: family size

- *hown*: home owner

- *inc*: income (in US $)

- *male*: male

- *marr*: married

- *pira*: participation in individual retirement account (IRA)

- *twoearn*: two earners

```{r, warning=F,message=F}
library(hdm)
library(DoubleML)
library(mlr3)
library(mlr3learners)
library(mlr3tuning)
library(ggplot2)
library(data.table)

# Get data
data(pension)
# Outcome
Y <-  pension$net_tfa
# Treatment
W <-  pension$p401
# Treatment
Z <-  pension$e401
# Create main effects matrix
X <-  model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension)


#collect all columns in a single data.table
data_pension <- data.table(cbind(X,W,Z,Y))
cols <- colnames(data_pension)
# remove columns Y,W,Z from cols
cols <- cols[!cols %in% c("Y", "W", "Z")]

```


## Partially linear regression model (PLR)

We have a regression problem because our outcome variable, net tfa, which represents net assets, is continuous in nature. Regression learners will be used to model the relationship between our predictor coefficients $X$ and the outcome variable $Y$.
The outcome variable $Y$ is modeled as a linear function of the policy variable $D$ and additional confounding factors $X$ in a partly linear regression model (PLR). The model's objective is to determine the causal relationship between $D$ and $Y$ while accounting for the impact of $X$. Since our outcome variable is continous a regression learner will always be utuilized for $Y$.
For $D$ it is appropiate to use a classification learner, since $D$ (`e401`) is a binary value.

### Model 1 (Linear & Logistic Regression) & 2 (Penalized Linear & Logistic Regression)

As a baseline, the first models will be a simple *Linear & Logistic Regression* for `ml_l` and `ml_l`. Our second model will be a **Penalized** *Linear & Logistic Regression*. This will serve as a baseline as they're normally the "simplest" type of model to implement.
Documentation to all available models can be found on [mlr3learners](https://mlr-org.com/learners.html)

```{r}

set.seed(123)
lgr::get_logger("mlr3")$set_threshold("warn")

obj_dml_data <- double_ml_data_from_data_frame(data_pension,
                                        y_col = "Y",
                                        d_cols = "W",
                                        x_cols = cols)


# Version 1 Linear Regression & Logistic Regression
ml_l_plr_1 <-  lrn("regr.lm")
ml_m_plr_1 <-  lrn("classif.log_reg")


#Version 2 Penalized Linear & Logistic Regression
ml_l_plr_2 <- lrn("regr.glmnet", s = 1/ncol(X)) #added rule of thumb lambda
ml_m_plr_2 <- lrn("classif.glmnet", s=1/ncol(X))




dml_plr_obj_1 <- DoubleMLPLR$new(obj_dml_data,
                               ml_l_plr_1,
                               ml_m_plr_1)
dml_plr_obj_1$fit()
print(dml_plr_obj_1)




# Initialize an object of class DoubleMLPLR
dml_plr_obj_2 <- DoubleMLPLR$new(obj_dml_data,
                                       ml_l_plr_2,
                                       ml_m_plr_2)

# Estimate the treatment effect
dml_plr_obj_2$fit()
print(dml_plr_obj_2)


```


## Partially linear IV regression model (PLIV)

For PLIV we had an additional Variable $Z$ that denotes one or more instrumental variables. For our example, we have `e401` which denotes the elligibility to the program. This is a logical value, but this doesn't mean we can use classification for our learner $Z$. Based on the documentation, PLIV's `ml_r` learner determines the conditional expectation of $Y$ and $D|W$ given $X$. Which makes it a regression problem.If I understood it correctly, the `ml_m` model, PLIV uses $Z$ to account for endogeneity - unobserved factors that affect both $D$ and $Y$. Which is why it also needs a regression learner.  
*This was a bit hard to find information on, please could you inform me if what I wrote is incorrect.This model doesn't accept any classification learners and this is all the information I could find on why.*

### Model 3 (K-Nearest Neighbor) & 4 (Neural Network)

Regression issues can be addressed using K-Nearest Neighbor (KNN) and Neural Network learners. *KNN* is a non-parametric algorithm that makes no assumptions about the data being used as input. *Neural Networks* are adaptable models that can more efficiently model complex data than conventional techniques.

```{r}
library(kknn)
library(nnet)

set.seed(123)

obj_dml_data_z <- DoubleMLData$new(data_pension,
                                     y_col="Y",
                                     d_col = "W",
                                     z_cols= "Z",
                                     x_cols = cols)
#version 1 K Nearest Neighbour 
ml_l_pliv_1 <- lrn("regr.kknn")
ml_m_pliv_1 <- lrn("regr.kknn")
ml_r_pliv_1 <- lrn("regr.kknn")


#Version 2 Neural Network
ml_l_pliv_2 <- lrn("regr.nnet")
ml_m_pliv_2 <- lrn("regr.nnet")
ml_r_pliv_2 <- lrn("regr.nnet")

#Version 1
dml_pliv_obj_1 <- DoubleMLPLIV$new(obj_dml_data_z,
                                ml_l_pliv_1,
                                ml_m_pliv_1,
                                ml_r_pliv_1)
dml_pliv_obj_1$fit()
print(dml_pliv_obj_1)


#Version 2
dml_pliv_obj_2 <- DoubleMLPLIV$new(obj_dml_data_z,
                                 ml_l_pliv_2,
                                 ml_m_pliv_2,
                                 ml_r_pliv_2)
#suppress nnet output in fit (sink to empty file)
sink("null")
dml_pliv_obj_2$fit()
sink()

print(dml_pliv_obj_2)

```


## Interactive regression model for ATE (IRM)
For IRM we have an outcome variable Y and a binary treatment variable D. The model takes the form $Y = g_0(D,X) + U$, $D = m_0(X) + V$, with $E[U|X,D]=0$ and $E[V|X] = 0$
Based on the documentation, IRM use machine learning techniques to calculate Neyman orthogonal scoring function and estimate nuisance functions. Through the use of Double Machine Learning techniques, this enables us to estimate causal effects.
Since $D|W$ is a binary treament variable, we can use classification learners for `ml_m`


### Model 5 (Support Vector Machine SVM) & 6 (XGBoost)

For the IRM Model, there's now an option to utilize both regression and classification learners. 
An effective technique for classification and regression issues is SVM. Gradient boosted decision trees are implemented using XGBoost, which is designed to be fast and effective in terms of implementation and performance.

```{r}
library(xgboost)
library(e1071) #for SVM
lgr::get_logger("mlr3")$set_threshold("warn") #change to "info" to see iterations

set.seed(123)

obj_dml_data <-  DoubleMLData$new(data_pension,
                                y_col="Y",
                                d_cols="W",
                                x_cols = cols)
#Version 1 SVM
ml_g_irm_1 <-  lrn("regr.svm")
ml_m_irm_1 <-  lrn("classif.svm")

#Version 2 XGBoost
ml_g_irm_2 <-  lrn("regr.xgboost")
ml_m_irm_2 <-  lrn("classif.xgboost")

#Version 1 Takes a bit longer to run
dml_irm_obj_1 <- DoubleMLIRM$new(obj_dml_data,
                                 ml_g_irm_1,
                                 ml_m_irm_1)
dml_irm_obj_1$fit()
print(dml_irm_obj_1)


#Version 2
dml_irm_obj_2 <- DoubleMLIRM$new(obj_dml_data,
                                 ml_g_irm_2,
                                 ml_m_irm_2)
dml_irm_obj_2$fit()
print(dml_irm_obj_2)

```




## Interactive IV model for LATE (IIVM)

Interactive IV Model (IIVM) comparable to the Interactive Regression Model (IRM), but adds the binary instrumental variable $Z$. With $E[U|X,D,Z]=0$ and $E[V|X,Z]=0$, the model has the following formulas: $Y = g 0(D,X) + U$, $D = m 0(Z,X) + V$.
Since Both $D|W$ and $Z$ are both binary values, we can utilize classification learners for `ml_m` and `ml_r`.

### Model 7 (Random Forrest) & 8 (Combination)

As a last learner, I elected to utilize the *Random Forest* learner.It is a meta-estimator that employs averaging to increase predictive accuracy and reduce overfitting after fitting numerous decision tree classifiers to different subsamples of the dataset.

Additionally I wanted to experiment with a combination of different models. So I utilized a *Random Forest* regression learner and a *XGBoost* classification learners, since it performed well in Model 5. Additionally, both models are a type of Decision tree, so the assumption can be made that a combination of the 2 could bare great results.


```{r}

set.seed(123)

obj_dml_data_z <- DoubleMLData$new(data_pension,
                                y_col="Y",
                                d_cols="W",
                                z_cols="Z",
                                x_cols = cols)

#Version 1 Random Forrest 
ml_g_iivm_1 <-  lrn("regr.ranger", num.trees = 100, mtry = ncol(X)-1, min.node.size = 2, max.depth = 5)
ml_m_iivm_1 <-  lrn("classif.ranger", num.trees = 100, mtry = ncol(X)-1, min.node.size = 2, max.depth = 5)
ml_r_iivm_1 <-  ml_m_iivm_1$clone()

#Version 2 Combination
ml_g_iivm_2 <-  lrn("regr.ranger")
ml_m_iivm_2 <-  lrn("classif.xgboost")
ml_r_iivm_2 <-  lrn("classif.xgboost")

#Version 1
dml_iivm_obj_1 <-  DoubleMLIIVM$new(obj_dml_data_z, 
                                    ml_g_iivm_1,
                                    ml_m_iivm_1,
                                    ml_r_iivm_1)
dml_iivm_obj_1$fit()
print(dml_iivm_obj_1)


#Version 2
dml_iivm_obj_2 <-  DoubleMLIIVM$new(obj_dml_data_z, 
                                    ml_g_iivm_2,
                                    ml_m_iivm_2,
                                    ml_r_iivm_2)
dml_iivm_obj_2$fit()
print(dml_iivm_obj_2)


```



## Model Evaluations

```{r}

set.seed(123)

dml_objs <- list(dml_plr_obj_1,dml_plr_obj_2,
                 dml_pliv_obj_1,dml_pliv_obj_2,
                 dml_irm_obj_1,dml_irm_obj_2,
                 dml_iivm_obj_1,dml_iivm_obj_2
                 )
names <-c("PLR", "PLIV", "IRM", "IIVM")
model_names <- c(rep(names, each = 2),"IIVM_HPT")


point_estimates <- sapply(dml_objs, function(x) x$coef)
std_errors <- sapply(dml_objs, function(x) x$se)

#Add p-values for stat significance
p_values <- sapply(dml_objs, function(x) x$pval)

# point_estimates <- sapply(dml_objs, function(x) x$coef["W"])
# std_errors <- sapply(dml_objs, function(x) x$se["W"])


# calculate 95% confidence intervals
lower_bounds <- point_estimates - 1.96 * std_errors
upper_bounds <- point_estimates + 1.96 * std_errors


# create a data frame for plotting and model description
df <- data.frame(
    model = paste0("Model ", seq_along(dml_objs)," ",
                   model_names[1:length(dml_objs)]),
    estimate = point_estimates,
    lower = lower_bounds,
    upper = upper_bounds,
    p_value = p_values
)


# Determine the narrowest confidence Interval
# calculate the width of each confidence interval
ci_widths <- upper_bounds - lower_bounds
# find the index of the model with the slimest confidence interval width
best_model_idx <- which.min(ci_widths)
# get the name of the most secure model
best_model_name <- df$model[best_model_idx]
best_model_text <- paste0("The model with the narrowest confidence interval is: ", best_model_name)

# plot point estimates with error bars representing the confidence intervals
ggplot(df, aes(x=model, y=estimate)) +
    geom_point(aes(color=model), size=3) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2, color="grey80") +
    scale_color_brewer(palette="Set1") +
    theme_light() +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.text.x = element_text(angle = 90)) + # rotate x-axis labels
    ggtitle("Model Evaluation") +
    labs(subtitle = best_model_text)+
    xlab("Model") +
    ylab("Point Estimate")

ci_widths_df <- data.frame(model = df$model, ci_width = ci_widths)
ci_widths_df
```

```{r}

set.seed(123)

# find the index of the model with the smallest p-value
best_model_idx <- which.min(df$p_value)

# get the name of the best model
best_model_name <- df$model[best_model_idx]

# create text for subtitle
best_model_text <- paste0("The model with highest statistical significance is: ", best_model_name)

# plot p-values for each model
ggplot(df, aes(x=model, y=p_value)) +
    geom_point(aes(color=model), size=3) +
    scale_color_brewer(palette="Set1") +
    theme_light() +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.text.x = element_text(angle = 90)) +
    ggtitle("Model Evaluation (p-values)") +
    labs(subtitle = best_model_text) + # add subtitle
    xlab("Model") +
    ylab("P-value")

p_value_df <- data.frame(model = df$model, p_value = df$p_value)
p_value_df <- p_value_df[order(p_value_df$p_value),]
p_value_df
```

A model may provide the best accurate estimate of the treatment impact if it has the narrowest confidence interval among the other models being compared. As a result, employing this model as opposed to other models, there is less doubt over the actual value of the treatment impact.
Based on this, we will now perform Hyperparameter tuning to determine if we can improve the model.


## Final Model (Hyperparameter Tunining)

In the Evaluation we can observe that the best performing model in terms of Confidence intervall is **Model 6 IRM**. In terms of statistical significance on the other hand it ranks in the lower half. With help of the `mlr3tuning` package I will now attempt to improve the model with Hyper Parameter Tuning for the Regression learner mainly.  
*For the Classification Learner it is unfortunately a bit more complicated (W needs to be of type factor) and when I attempted to work around this, the model did not improve.*

### XGBOOST Hyper Parameter Tuning 

```{r}

# Set log level to 'warn' to mute info messages
lgr::get_logger("mlr3")$set_threshold("warn")
set.seed(123)


library(DoubleML)
library(mlr3)
library(mlr3tuning)

# Set up the data
obj_dml_data_9 <- DoubleMLData$new(data_pension,
                                      y_col = "Y",
                                      d_col = "W",
                                      x_cols = cols)

#The instance classifier for classifier learner needs a factor variable
# data_pension_tuning <- data_pension
# data_pension_tuning$W <- as.factor(data_pension_tuning$W)
# obj_dml_data_9_factor <- DoubleMLData$new(data_pension_tuning,
#                                       y_col = "Y",
#                                       d_col = "W",
#                                       x_cols = cols)



# Set up the learners
learner_regr <- lrn("regr.xgboost")
learner_classif <- lrn("classif.xgboost")

# Define the search space for hyperparameters
param_set_regr <- ParamSet$new(list(
  ParamDbl$new("eta", lower = 0.1, upper = 1),
  ParamInt$new("max_depth", lower = 1, upper = 10),
  ParamInt$new("nrounds", lower = 50, upper = 500)
))

# Set up the tuning instances
instance_regr <- TuningInstanceSingleCrit$new(
    task=TaskRegr$new(id="pliv_task",
                      backend=obj_dml_data_9$data,
                      target="Y"),
    learner=learner_regr,
    resampling=rsmp("cv", folds=5),
    measure=msr("regr.mse"),
    search_space=param_set_regr,
    terminator=trm("evals", n_evals=10)
)

# Run the tuning
tuner_regr <- tnr("grid_search")
tuner_regr$optimize(instance_regr)


# Extract the best hyperparameters
best_params_regr <- instance_regr$result_learner_param_vals

# Update the learners with the best hyperparameters
ml_g_irm_9 <- learner_regr$clone()
ml_g_irm_9$param_set$values <- best_params_regr

ml_m_irm_9 <- learner_classif$clone()

dml_irm_obj_9 <-DoubleMLIRM$new(obj_dml_data_9,
                               ml_g_irm_9,
                               ml_m_irm_9)

dml_irm_obj_9$fit()
print(dml_irm_obj_9)



```

## Evaluation of HPT Model

```{r}
set.seed(123)

dml_objs <- list(dml_plr_obj_1,dml_plr_obj_2,
                 dml_pliv_obj_1,dml_pliv_obj_2,
                 dml_irm_obj_1,dml_irm_obj_2,
                 dml_iivm_obj_1,dml_iivm_obj_2,
                 dml_irm_obj_9)
names <-c("PLR", "PLIV", "IRM", "IIVM")
model_names <- c(rep(names, each = 2),"IRM_HPT")


point_estimates <- sapply(dml_objs, function(x) x$coef)
std_errors <- sapply(dml_objs, function(x) x$se)

#Add p-values for stat significance
p_values <- sapply(dml_objs, function(x) x$pval)

# point_estimates <- sapply(dml_objs, function(x) x$coef["W"])
# std_errors <- sapply(dml_objs, function(x) x$se["W"])


# calculate 95% confidence intervals
lower_bounds <- point_estimates - 1.96 * std_errors
upper_bounds <- point_estimates + 1.96 * std_errors


# create a data frame for plotting and model description
df <- data.frame(
    model = paste0("Model ", seq_along(dml_objs)," ",
                   model_names[1:length(dml_objs)]),
    estimate = point_estimates,
    lower = lower_bounds,
    upper = upper_bounds,
    p_value = p_values
)


# Determine the narrowest confidence Interval
# calculate the width of each confidence interval
ci_widths <- upper_bounds - lower_bounds
# find the index of the model with the slimest confidence interval width
best_model_idx <- which.min(ci_widths)
# get the name of the most secure model
best_model_name <- df$model[best_model_idx]
best_model_text <- paste0("The model with the narrowest confidence interval is: ", best_model_name)

# plot point estimates with error bars representing the confidence intervals
ggplot(df, aes(x=model, y=estimate)) +
    geom_point(aes(color=model), size=3) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2, color="grey80") +
    scale_color_brewer(palette="Set1") +
    theme_light() +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.text.x = element_text(angle = 90)) + # rotate x-axis labels
    ggtitle("Model Evaluation") +
    labs(subtitle = best_model_text)+
    xlab("Model") +
    ylab("Point Estimate")

ci_widths_df <- data.frame(model = df$model, ci_width = ci_widths)
ci_widths_df

```

```{r}

set.seed(123)

# find the index of the model with the smallest p-value
best_model_idx <- which.min(df$p_value)

# get the name of the best model
best_model_name <- df$model[best_model_idx]

# create text for subtitle
best_model_text <- paste0("The model with highest statistical significance is: ", best_model_name)

# plot p-values for each model
ggplot(df, aes(x=model, y=p_value)) +
    geom_point(aes(color=model), size=3) +
    scale_color_brewer(palette="Set1") +
    theme_light() +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.text.x = element_text(angle = 90)) +
    ggtitle("Model Evaluation (p-values)") +
    labs(subtitle = best_model_text) + # add subtitle
    xlab("Model") +
    ylab("P-value")

p_value_df <- data.frame(model = df$model, p_value = df$p_value)
p_value_df <- p_value_df[order(p_value_df$p_value),]
p_value_df

```

As we can see, our **Model 9 IRM_HPT** performs the best in terms of Confidence interval and also delivers the 2nd highest statistical significance.Thus thorougly improving the baseline XGBoost IRM model.


