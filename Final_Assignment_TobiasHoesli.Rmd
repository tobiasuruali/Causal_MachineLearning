---
title: "Assignment"
subtitle: "Causal ML Luzern"
author: "Tobias Hösli"
date: "`r format(Sys.time(), '%m/%y')`"
output: 
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
---

<br>

# Theory recap: Double ML (20 points)
*Briefly describe in your own words (and potentially formulas) the general idea of Double Machine Learning. What is the crucial property that makes it work? What should the nuisance parameter predictions fulfill for Double ML to work?*

<br>

Double Machine Learning (DML) is a method for estimating treatment effects in the presence of many observed covariates. Let $Y$ denote the outcome variable, $W|D$ denote the treatment variable and $X$ denote the vector of covariates. The goal of DML is to estimate the average treatment effect $\theta_0 = E[Y(1) - Y(0)]$, where $Y(d)$ denotes the potential outcome under treatment level $d \in {0, 1}$.

Classical statistical approaches may not be applicable when there are many covariates or when their effect on the treatment and outcome cannot be satisfactorily modeled by parametric functions. DML addresses this issue by using machine learning methods to estimate nuisance functions that allow us to isolate the effect of the treatment from other factors.

The first step in DML is to estimate two nuisance functions: (1) The conditional expectation function of the outcome given covariates and treatment: $\hat{\mu}(X,D) = E[Y|X,D]$; and (2) The propensity score function: $\hat{e}(X) = E[D|X]$. To account for confounding variables and lessen bias in treatment effect estimations, these nuisance parameters are used and can be estimated using flexible machine learning methods.

Next, we compute an orthogonal score function based on these estimates. For example, in a  Interactive regression model (IRM) with a binary treatment variable, we can use Neyman’s orthogonal score function: *($W$ is the data, $Y$ the outcome variable, $D$ the treatment variable and $X$ covariates)*

$$
\psi(W;\theta,\eta) := (g(1,X) - g(0,X)) + D(Y-g(1,X)) m(X) - (1-D)(Y-g(0,X)) 1-m(X)-\theta
$$
The average treatment effect (ATE) can then be estimated as (IRM):

$$
\begin{aligned}
\hat{\tau}_{ATE} &= \frac{1}{n}\sum_{i=1}^n \psi(W_i;\theta,\eta) \\
\psi(W ; \theta, \eta) & :=g(1, X)-g(0, X)+\frac{D(Y-g(1, X))}{m(X)}-\frac{(1-D)(Y-g(0, X))}{1-m(x)}-\theta \\
& =\psi_a(W ; \eta) \theta+\psi_b(W ; \eta)
\end{aligned}
$$

One advantage of DML over other methods for estimating treatment effects is its ability to handle high-dimensional data and non-parametric relationships between covariates and outcomes. This makes it well-suited for applications where there are many potential confounders or where it is difficult to specify a parametric model for their relationship with outcomes.

Double Machine Learning (DML) provides a powerful tool for estimating treatment effects in situations where traditional statistical approaches may not be applicable due to high dimensionality or complex relationships between covariates and outcomes.But for DML to work,predictions of nuisance parameters must be low-biased and meet regularity conditions. 


<br>


# Practice task: Average effects with and without instrument (60 points)

## 401(k) dataset 

We again use the data of the `hdm` package. The data was used in [Chernozhukov and Hansen (2004)](https://direct.mit.edu/rest/article/86/3/735/57586/The-Effects-of-401-K-Participation-on-the-Wealth). Their paper investigates the effect of participation in the employer-sponsored 401(k) retirement savings plan (*p401*) on net assets (*net_tfa*). Since then, the data was used to showcase many new methods. It is not the most comprehensive dataset with basically ten covariates/regressors/predictors:

- *age*: age

- *db*: defined benefit pension

- *educ*: education (in years)

- *fsize*: family size

- *hown*: home owner

- *inc*: income (in US $)

- *male*: male

- *marr*: married

- *pira*: participation in individual retirement account (IRA)

- *twoearn*: two earners

```{r, warning=F,message=F}
library(hdm)
library(DoubleML)
library(mlr3)
library(mlr3learners)
library(mlr3tuning)
library(ggplot2)
library(data.table)

# Get data
data(pension)
# Outcome
Y <-  pension$net_tfa
# Treatment
W <-  pension$p401
# Treatment
Z <-  pension$e401
# Create main effects matrix
X <-  model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension)


#collect all columns in a single data.table
data_pension <- data.table(cbind(X,W,Z,Y))
cols <- colnames(data_pension)
# remove columns Y,W,Z from cols
cols <- cols[!cols %in% c("Y", "W", "Z")]

```


## Partially linear regression model (PLR)

We have a regression problem because our outcome variable, net tfa, which represents net assets, is continuous in nature. Regression learners will be used to model the relationship between our predictor coefficients $X$ and the outcome variable $Y$.
The outcome variable $Y$ is modeled as a linear function of the policy variable $D$ and additional confounding factors $X$ in a partly linear regression model (PLR). The model's objective is to determine the causal relationship between $D$ and $Y$ while accounting for the impact of $X$. Since our outcome variable is continous a regression learner will always be utuilized for $Y$.
For $D$ it is appropiate to use a classification learner, since $D$ (`e401`) is a binary value.

As a baseline, the first models will be a simple Linear & Logistic Regression for `ml_l` and `ml_l`. Our second model will be a **Penalized** Linear & Logistic Regression. This will serve as a baseline as they're normally the "simplest" type of model to implement.
Documentation to all available models can be found on [mlr3learners](https://mlr-org.com/learners.html)

### Model 1 (Linear & Logistic Regression) & 2 (Penalized Linear & Logistic Regression)

```{r}

set.seed(123)
lgr::get_logger("mlr3")$set_threshold("warn")

obj_dml_data <- double_ml_data_from_data_frame(data_pension,
                                        y_col = "Y",
                                        d_cols = "W",
                                        x_cols = cols)


# Version 1 Linear Regression & Logistic Regression
ml_l_plr_1 <-  lrn("regr.lm")
ml_m_plr_1 <-  lrn("classif.log_reg")


#Version 2 Penalized Linear & Logistic Regression
ml_l_plr_2 <- lrn("regr.glmnet", s = 1/ncol(X)) #added rule of thumb lambda
ml_m_plr_2 <- lrn("classif.glmnet", s=1/ncol(X))




dml_plr_obj_1 <- DoubleMLPLR$new(obj_dml_data,
                               ml_l_plr_1,
                               ml_m_plr_1)
dml_plr_obj_1$fit()
print(dml_plr_obj_1)




# Initialize an object of class DoubleMLPLR
dml_plr_obj_2 <- DoubleMLPLR$new(obj_dml_data,
                                       ml_l_plr_2,
                                       ml_m_plr_2)

# Estimate the treatment effect
dml_plr_obj_2$fit()
print(dml_plr_obj_2)


```


## Partially linear IV regression model (PLIV)

For PLIV we had an additional Variable $Z$ that denotes one or more instrumental variables. For our example, we have `e401` which denotes the elligibility to the program. This is a logical value, but this doesn't mean we can use classification for our learner $Z$. Based on the documentation, PLIV's `ml_r` learner determines the conditional expectation of $Y$ and $D|W$ given $X$. Which makes it a regression problem.If I understood it correctly, the `ml_m` model, PLIV uses $Z$ to account for endogeneity - unobserved factors that affect both $D$ and $Y$. Which is why it also needs a regression learner.
*This was a bit hard to find information on, please could you inform me if what I wrote is incorrect.This model doesn't accept any classification learners and this is all the information I could find on why.*

### Model 3 (K-Nearest Neighbor) & 4 (Neural Network)

```{r}
library(kknn)
library(nnet)

set.seed(123)

obj_dml_data_z <- DoubleMLData$new(data_pension,
                                     y_col="Y",
                                     d_col = "W",
                                     z_cols= "Z",
                                     x_cols = cols)
#version 1 K Nearest Neighbour 
ml_l_pliv_1 <- lrn("regr.kknn")
ml_m_pliv_1 <- lrn("regr.kknn")
ml_r_pliv_1 <- lrn("regr.kknn")


#Version 2 Neural Network
ml_l_pliv_2 <- lrn("regr.nnet")
ml_m_pliv_2 <- lrn("regr.nnet")
ml_r_pliv_2 <- lrn("regr.nnet")

#Version 1
dml_pliv_obj_1 <- DoubleMLPLIV$new(obj_dml_data_z,
                                ml_l_pliv_1,
                                ml_m_pliv_1,
                                ml_r_pliv_1)
dml_pliv_obj_1$fit()
print(dml_pliv_obj_1)


#Version 2
dml_pliv_obj_2 <- DoubleMLPLIV$new(obj_dml_data_z,
                                 ml_l_pliv_2,
                                 ml_m_pliv_2,
                                 ml_r_pliv_2)
#suppress nnet output in fit (sink to empty file)
sink("null")
dml_pliv_obj_2$fit()
sink()

print(dml_pliv_obj_2)

```


## Interactive regression model for ATE (IRM)




### Model 5 (Support Vector Machine SVM) & 6 (XGBoost)

```{r}
library(xgboost)
library(e1071) #for SVM
lgr::get_logger("mlr3")$set_threshold("warn") #change to "info" to see iterations

set.seed(123)

obj_dml_data <-  DoubleMLData$new(data_pension,
                                y_col="Y",
                                d_cols="W",
                                x_cols = cols)
#Version 1 SVM
ml_g_irm_1 <-  lrn("regr.svm")
ml_m_irm_1 <-  lrn("classif.svm")

#Version 2 XGBoost
ml_g_irm_2 <-  lrn("regr.xgboost")
ml_m_irm_2 <-  lrn("classif.xgboost")

#Version 1 Takes a bit longer to run
dml_irm_obj_1 <- DoubleMLIRM$new(obj_dml_data,
                                 ml_g_irm_1,
                                 ml_m_irm_1)
dml_irm_obj_1$fit()
print(dml_irm_obj_1)


#Version 2
dml_irm_obj_2 <- DoubleMLIRM$new(obj_dml_data,
                                 ml_g_irm_2,
                                 ml_m_irm_2)
dml_irm_obj_2$fit()
print(dml_irm_obj_2)

```




## Interactive IV model for LATE (IIVM)

This model uses three different machine learning learners: ml_g_iivm_1, ml_m_iivm_1, and ml_r_iivm_1. All three learners are based on the random forest algorithm (ranger), but they serve different purposes in the IIVM model.

ml_g_iivm_1 is a regression learner (regr.ranger) that is used to estimate the so-called “nuisance function” g0 in the first stage of the IIVM model. This function captures the relationship between the instrument variable(s) Z and the covariates X.

ml_m_iivm_1 is a classification learner (classif.ranger) that is used to estimate the nuisance function m0 in the first stage of the IIVM model. This function captures the relationship between the treatment variable(s) W and the covariates X.

ml_r_iivm_1 is a clone of ml_m_iivm_1 and is also a classification learner (classif.ranger). It is used to estimate another nuisance function r0 in the second stage of the IIVM model. This function captures how well we can predict Y from Z after controlling for X.

All three learners are random forests with 100 trees (num.trees = 100) and use all but one of the available predictor variables at each split (mtry = ncol(X)-1). The minimum size of terminal nodes is set to 2 (min.node.size = 2) and trees can grow up to a maximum depth of 5 (max.depth = 5).


### Model 7 (Random Forrest) & 8 (Combination)

```{r}

set.seed(123)

obj_dml_data_z <- DoubleMLData$new(data_pension,
                                y_col="Y",
                                d_cols="W",
                                z_cols="Z",
                                x_cols = cols)

#Version 1 Random Forrest 
ml_g_iivm_1 <-  lrn("regr.ranger", num.trees = 100, mtry = ncol(X)-1, min.node.size = 2, max.depth = 5)
ml_m_iivm_1 <-  lrn("classif.ranger", num.trees = 100, mtry = ncol(X)-1, min.node.size = 2, max.depth = 5)
ml_r_iivm_1 <-  ml_m_iivm_1$clone()

#Version 2 Combination
ml_g_iivm_2 <-  lrn("regr.xgboost")
ml_m_iivm_2 <-  lrn("classif.ranger", num.trees = 100, mtry = ncol(X)-1, min.node.size = 2, max.depth = 5)
ml_r_iivm_2 <-  lrn("classif.ranger", num.trees = 100, mtry = ncol(X)-1, min.node.size = 2, max.depth = 5)

#Version 1
dml_iivm_obj_1 <-  DoubleMLIIVM$new(obj_dml_data_z, 
                                    ml_g_iivm_1,
                                    ml_m_iivm_1,
                                    ml_r_iivm_1)
dml_iivm_obj_1$fit()
print(dml_iivm_obj_1)


#Version 2
dml_iivm_obj_2 <-  DoubleMLIIVM$new(obj_dml_data_z, 
                                    ml_g_iivm_2,
                                    ml_m_iivm_2,
                                    ml_r_iivm_2)
dml_iivm_obj_2$fit()
print(dml_iivm_obj_2)


```



## Model Evaluations

```{r}

dml_objs <- list(dml_plr_obj_1,dml_plr_obj_2,
                 dml_pliv_obj_1,dml_pliv_obj_2,
                 dml_irm_obj_1,dml_irm_obj_2,
                 dml_iivm_obj_1,dml_iivm_obj_2
                 )
names <-c("PLR", "PLIV", "IRM", "IIVM")
model_names <- c(rep(names, each = 2),"IIVM_HPT")


point_estimates <- sapply(dml_objs, function(x) x$coef)
std_errors <- sapply(dml_objs, function(x) x$se)

#Add p-values for stat significance
p_values <- sapply(dml_objs, function(x) x$pval)

# point_estimates <- sapply(dml_objs, function(x) x$coef["W"])
# std_errors <- sapply(dml_objs, function(x) x$se["W"])


# calculate 95% confidence intervals
lower_bounds <- point_estimates - 1.96 * std_errors
upper_bounds <- point_estimates + 1.96 * std_errors


# create a data frame for plotting and model description
df <- data.frame(
    model = paste0("Model ", seq_along(dml_objs)," ",
                   model_names[1:length(dml_objs)]),
    estimate = point_estimates,
    lower = lower_bounds,
    upper = upper_bounds,
    p_value = p_values
)


# Determine the narrowest confidence Interval
# calculate the width of each confidence interval
ci_widths <- upper_bounds - lower_bounds
# find the index of the model with the slimest confidence interval width
best_model_idx <- which.min(ci_widths)
# get the name of the most secure model
best_model_name <- df$model[best_model_idx]
best_model_text <- paste0("The model with the narrowest confidence interval is: ", best_model_name)

# plot point estimates with error bars representing the confidence intervals
ggplot(df, aes(x=model, y=estimate)) +
    geom_point(aes(color=model), size=3) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2, color="grey80") +
    scale_color_brewer(palette="Set1") +
    theme_light() +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.text.x = element_text(angle = 90)) + # rotate x-axis labels
    ggtitle("Model Evaluation") +
    labs(subtitle = best_model_text)+
    xlab("Model") +
    ylab("Point Estimate")

ci_widths_df <- data.frame(model = df$model, ci_width = ci_widths)
ci_widths_df
```
```{r}

# find the index of the model with the smallest p-value
best_model_idx <- which.min(df$p_value)

# get the name of the best model
best_model_name <- df$model[best_model_idx]

# create text for subtitle
best_model_text <- paste0("The model with highest statistical significance is: ", best_model_name)

# plot p-values for each model
ggplot(df, aes(x=model, y=p_value)) +
    geom_point(aes(color=model), size=3) +
    scale_color_brewer(palette="Set1") +
    theme_light() +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.text.x = element_text(angle = 90)) +
    ggtitle("Model Evaluation (p-values)") +
    labs(subtitle = best_model_text) + # add subtitle
    xlab("Model") +
    ylab("P-value")

p_value_df <- data.frame(model = df$model, p_value = df$p_value)
p_value_df <- p_value_df[order(p_value_df$p_value),]
p_value_df
```

A model may provide the best accurate estimate of the treatment impact if it has the narrowest confidence interval among the other models being compared. As a result, employing this model as opposed to other models, there is less doubt over the actual value of the treatment impact.
Based on this, we will now perform Hyperparameter tuning to determine if we can improve the model.


## Final Model (Hyperparameter Tunining)


```{r}

library(DoubleML)
library(mlr3)
library(mlr3tuning)

# Set up the data
obj_dml_data_pliv <- DoubleMLData$new(data_pension,
                                      y_col = "Y",
                                      d_col = "W",
                                      z_cols = "Z",
                                      x_cols = cols)

# Set up the learner
learner <- lrn("regr.ranger")

# Define the search space for hyperparameters
param_set <- ParamSet$new(list(
  ParamInt$new("num.trees", lower = 10, upper = 500),
  ParamInt$new("mtry", lower = 1, upper = ncol(obj_dml_data_pliv$data)-1),
  ParamInt$new("min.node.size", lower = 1, upper = 10),
  ParamInt$new("max.depth", lower = 1, upper = 10)
))

# Set up the tuning instance
instance <- TuningInstanceSingleCrit$new(
  task = TaskRegr$new(id = "pliv_task", backend=obj_dml_data_pliv$data,
                      target="Y"),
  learner = learner,
  resampling = rsmp("cv", folds=5),
  measure=msr("regr.mse"),
  search_space=param_set,
  terminator=trm("evals", n_evals=25)
)

# Run the tuning
tuner <- tnr("grid_search")
tuner$optimize(instance)

# Extract the best hyperparameters
best_params <- instance$result_learner_param_vals

# Update the learners with the best hyperparameters
ml_l_pliv <- learner$clone()
ml_l_pliv$param_set$values <-  best_params
ml_m_pliv <- learner$clone()
ml_m_pliv$param_set$values <- best_params
ml_r_pliv <- learner$clone()
ml_r_pliv$param_set$values <- best_params

dml_pliv_obj <- DoubleMLPLIV$new(obj_dml_data_pliv,
                                 ml_l_pliv,
                                 ml_m_pliv,
                                 ml_r_pliv)

dml_pliv_obj$fit()
print(dml_pliv_obj)


```

#XGBOOST 

```{r}

# Set log level to 'warn' to mute info messages
# lgr::get_logger("mlr3")$set_threshold("warn")

library(DoubleML)
library(mlr3)
library(mlr3tuning)

# Set up the data
obj_dml_data_9 <- DoubleMLData$new(data_pension,
                                      y_col = "Y",
                                      d_col = "W",
                                      z_cols = "Z",
                                      x_cols = cols)

#The instance classifier for classifier learner needs a factor variable
data_pension_tuning <- data_pension
data_pension_tuning$W <- as.factor(data_pension_tuning$W)
obj_dml_data_9_factor <- DoubleMLData$new(data_pension_tuning,
                                      y_col = "Y",
                                      d_col = "W",
                                      z_cols = "Z",
                                      x_cols = cols)



# Set up the learners
learner_regr <- lrn("regr.xgboost")
learner_classif <- lrn("classif.xgboost")

# Define the search space for hyperparameters
param_set_regr <- ParamSet$new(list(
  ParamDbl$new("eta", lower = 0.1, upper = 1),
  ParamInt$new("max_depth", lower = 1, upper = 10),
  ParamInt$new("nrounds", lower = 50, upper = 500)
))

param_set_classif <- ParamSet$new(list(
  ParamDbl$new("eta", lower = 0.1, upper = 1),
  ParamInt$new("max_depth", lower = 1, upper = 10),
  ParamInt$new("nrounds", lower = 50, upper = 500)
))

# Set up the tuning instances
instance_regr <- TuningInstanceSingleCrit$new(
    task=TaskRegr$new(id="pliv_task",
                      backend=obj_dml_data_9$data,
                      target="Y"),
    learner=learner_regr,
    resampling=rsmp("cv", folds=5),
    measure=msr("regr.mse"),
    search_space=param_set_regr,
    terminator=trm("evals", n_evals=5)
)

instance_classif <- TuningInstanceSingleCrit$new(
    task=TaskClassif$new(id="pliv_task",
                         backend=obj_dml_data_9_factor$data,
                         target="W"),
    learner=learner_classif,
    resampling=rsmp("cv", folds=5),
    measure=msr("classif.ce"),
    search_space=param_set_classif,
    terminator=trm("evals", n_evals=5)
)

# Run the tuning
tuner_regr <- tnr("grid_search")
tuner_regr$optimize(instance_regr)

tuner_classif <- tnr("grid_search")
tuner_classif$optimize(instance_classif)

# Extract the best hyperparameters
best_params_regr <- instance_regr$result_learner_param_vals
best_params_classif <- instance_classif$result_learner_param_vals

# Update the learners with the best hyperparameters
ml_g_irm_9 <- learner_regr$clone()
ml_g_irm_9$param_set$values <- best_params_regr

ml_m_irm_9 <- learner_classif$clone()
ml_m_irm_9$param_set$values<-best_params_classif

ml_r_irm_9 <- ml_m_irm_9$clone()

dml_irm_obj_9 <-DoubleMLIIVM$new(obj_dml_data_9,
                               ml_g_irm_9,
                               ml_m_irm_9,
                               ml_r_irm_9)

dml_irm_obj_9$fit()
print(dml_irm_obj_9)



```


```{r}
dml_objs <- list(dml_plr_obj_1,dml_plr_obj_2,
                 dml_pliv_obj_1,dml_pliv_obj_2,
                 dml_irm_obj_1,dml_irm_obj_2,
                 dml_iivm_obj_1,dml_iivm_obj_2,
                 dml_irm_obj_9)
names <-c("PLR", "PLIV", "IRM", "IIVM")
model_names <- c(rep(names, each = 2),"IIVM_HPT")


point_estimates <- sapply(dml_objs, function(x) x$coef)
std_errors <- sapply(dml_objs, function(x) x$se)

#Add p-values for stat significance
p_values <- sapply(dml_objs, function(x) x$pval)

# point_estimates <- sapply(dml_objs, function(x) x$coef["W"])
# std_errors <- sapply(dml_objs, function(x) x$se["W"])


# calculate 95% confidence intervals
lower_bounds <- point_estimates - 1.96 * std_errors
upper_bounds <- point_estimates + 1.96 * std_errors


# create a data frame for plotting and model description
df <- data.frame(
    model = paste0("Model ", seq_along(dml_objs)," ",
                   model_names[1:length(dml_objs)]),
    estimate = point_estimates,
    lower = lower_bounds,
    upper = upper_bounds,
    p_value = p_values
)


# Determine the narrowest confidence Interval
# calculate the width of each confidence interval
ci_widths <- upper_bounds - lower_bounds
# find the index of the model with the slimest confidence interval width
best_model_idx <- which.min(ci_widths)
# get the name of the most secure model
best_model_name <- df$model[best_model_idx]
best_model_text <- paste0("The model with the narrowest confidence interval is: ", best_model_name)

# plot point estimates with error bars representing the confidence intervals
ggplot(df, aes(x=model, y=estimate)) +
    geom_point(aes(color=model), size=3) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2, color="grey80") +
    scale_color_brewer(palette="Set1") +
    theme_light() +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.text.x = element_text(angle = 90)) + # rotate x-axis labels
    ggtitle("Model Evaluation") +
    labs(subtitle = best_model_text)+
    xlab("Model") +
    ylab("Point Estimate")

ci_widths_df <- data.frame(model = df$model, ci_width = ci_widths)
ci_widths_df

```

```{r}

# find the index of the model with the smallest p-value
best_model_idx <- which.min(df$p_value)

# get the name of the best model
best_model_name <- df$model[best_model_idx]

# create text for subtitle
best_model_text <- paste0("The model with highest statistical significance is: ", best_model_name)

# plot p-values for each model
ggplot(df, aes(x=model, y=p_value)) +
    geom_point(aes(color=model), size=3) +
    scale_color_brewer(palette="Set1") +
    theme_light() +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
          axis.text.x = element_text(angle = 90)) +
    ggtitle("Model Evaluation (p-values)") +
    labs(subtitle = best_model_text) + # add subtitle
    xlab("Model") +
    ylab("P-value")

p_value_df <- data.frame(model = df$model, p_value = df$p_value)
p_value_df <- p_value_df[order(p_value_df$p_value),]
p_value_df

```




