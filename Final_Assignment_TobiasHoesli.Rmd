---
title: "Assignment"
subtitle: "Causal ML Luzern"
author: "Tobias HÃ¶sli"
date: "`r format(Sys.time(), '%m/%y')`"
output: 
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
---

<br>

# Theory recap: Double ML (20 points)
Briefly describe in your own words (and potentially formulas) the general idea of Double Machine Learning. What is the crucial property that makes it work? What should the nuisance parameter predictions fulfill for Double ML to work?

<br>
<br>


# Practice task: Average effects with and without instrument (60 points)

## 401(k) dataset again

We again use the data of the `hdm` package. The data was used in [Chernozhukov and Hansen (2004)](https://direct.mit.edu/rest/article/86/3/735/57586/The-Effects-of-401-K-Participation-on-the-Wealth). Their paper investigates the effect of participation in the employer-sponsored 401(k) retirement savings plan (*p401*) on net assets (*net_tfa*). Since then, the data was used to showcase many new methods. It is not the most comprehensive dataset with basically ten covariates/regressors/predictors:

- *age*: age

- *db*: defined benefit pension

- *educ*: education (in years)

- *fsize*: family size

- *hown*: home owner

- *inc*: income (in US $)

- *male*: male

- *marr*: married

- *pira*: participation in individual retirement account (IRA)

- *twoearn*: two earners

```{r, warning=F,message=F}
library(hdm)

# Get data
data(pension)
# Outcome
Y <-  pension$net_tfa
# Treatment
W <-  pension$p401
# Treatment
Z <-  pension$e401
# Create main effects matrix
X <-  model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension)


#add 

data_pension <- data.table(cbind(X,W,Z,Y))
cols <- colnames(data_pension)
# remove columns Y,W,Z from cols
cols <- cols[!cols %in% c("Y", "W", "Z")]

```


## Partially linear regression model (PLR)

Explanation of Estimator ML Methods

### Version 1 (Regression Random Forrest)

```{r}

library(DoubleML)
library(mlr3)
library(mlr3learners)

set.seed(123)

lgr::get_logger("mlr3")$set_threshold("warn")
learner <-  lrn("regr.ranger", num.trees = 100, mtry = ncol(X)-1, min.node.size = 2, max.depth = 5)
# learner <- lrn("regr.ranger")
ml_l = learner$clone()
ml_m = learner$clone()

obj_dml_data <- double_ml_data_from_data_frame(data_pension,
                                        y_col = "Y",
                                        d_cols = "W",
                                        x_cols = cols)


dml_plr_obj <- DoubleMLPLR$new(obj_dml_data, ml_l, ml_m)
dml_plr_obj$fit()
print(dml_plr_obj)


```
### Version 2 (Regression and Classification Random Forest)


```{r}

library(DoubleML)
library(mlr3)
library(mlr3learners)

# Set up the data as a DoubleMLData object
dml_data <- double_ml_data_from_data_frame(pension,
                                        y_col = "net_tfa",
                                        d_cols = "p401",
                                        x_cols = cols)

# Set up the machine learning methods for m & g
learner_l <- lrn("regr.ranger")
learner_m <- lrn("classif.ranger")

# Initialize an object of class DoubleMLPLR
dml_plr_obj_classif <- DoubleMLPLR$new(dml_data,
                               ml_l = learner_l,
                               ml_m = learner_m)

# Estimate the treatment effect
dml_plr_obj_classif$fit()
dml_plr_obj_classif$summary()
print(dml_plr_obj_classif)

```

## Partially linear IV regression model (PLIV)


### Version 1 

```{r}

set.seed(123)

learner = lrn("regr.ranger", num.trees = 100, mtry = ncol(X)-1, min.node.size = 2, max.depth = 5)
ml_l_pliv = learner$clone()
ml_m_pliv = learner$clone()
ml_r_pliv = learner$clone()

obj_dml_data_pliv = DoubleMLData$new(data_pension,
                                     y_col="Y",
                                     d_col = "W",
                                     z_cols= "Z",
                                     x_cols = cols)
dml_pliv_obj = DoubleMLPLIV$new(obj_dml_data_pliv, ml_l_pliv, ml_m_pliv, ml_r_pliv)
dml_pliv_obj$fit()
print(dml_pliv_obj)


```
### Version 2 

```{r}

set.seed(123)

#use different classifier
learner = lrn("regr.ranger", num.trees = 100, mtry = ncol(X)-1, min.node.size = 2, max.depth = 5)
ml_l_pliv = learner$clone()
ml_m_pliv = learner$clone()
ml_r_pliv = learner$clone()

obj_dml_data_pliv = DoubleMLData$new(data_pension,
                                     y_col="Y",
                                     d_col = "W",
                                     z_cols= "Z",
                                     x_cols = cols)
dml_pliv_obj = DoubleMLPLIV$new(obj_dml_data_pliv, ml_l_pliv, ml_m_pliv, ml_r_pliv)
dml_pliv_obj$fit()
print(dml_pliv_obj)




```


## Interactive regression model for ATE (IRM)

### Version 1 



### Version 2 



## Interactive IV model for LATE (IIVM)


### Version 1 

### Version 2


## Plot Model Evaluations

```{r}

dml_objs <- list(dml_plr_obj_classif, dml_pliv_obj)

point_estimates <- sapply(dml_objs, function(x) x$coef)
std_errors <- sapply(dml_objs, function(x) x$se)
# point_estimates <- sapply(dml_objs, function(x) x$coef["W"])
# std_errors <- sapply(dml_objs, function(x) x$se["W"])


# calculate 95% confidence intervals
lower_bounds <- point_estimates - 1.96 * std_errors
upper_bounds <- point_estimates + 1.96 * std_errors

# create a data frame for plotting
df <- data.frame(
    model = paste0("Model ", seq_along(dml_objs)),
    estimate = point_estimates,
    lower = lower_bounds,
    upper = upper_bounds
)

# plot point estimates with error bars representing the confidence intervals
ggplot(df, aes(x=model, y=estimate)) +
    geom_point() +
    geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2) +
    xlab("Model") +
    ylab("Point Estimate")



```





